{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f72679-d1d8-4dbe-90e6-df28aa84fd99",
   "metadata": {},
   "source": [
    "# The Need for Statistical Modelling\n",
    "Before we begin discussing the techniques used to analyse fMRI data from a single subject, it is worth taking some time to understand the motivation for statistical modelling.\n",
    "\n",
    "## Why Do We Want to Model fMRI Data?\n",
    "A statistical model, whether applied to fMRI data or some other form of data, is built upon the assumption that there are effects that are *constant* across observations of the phenomena of interest. In the context of fMRI, this involves assuming that there are BOLD signal changes in respoonse to the experimental manipulations that are consistent across observations and across subjects. However, because other factors will influence the measurements of these effects, there will be a degree of *variability* from measurement-to-measurement and from subject-to-subject. This is quantified in terms of assuming that each measurement represents a draw from some probability distribution with fixed parameters. By using the available data to estimate those parameters, a statistical model allows us to separate the effects of interest from the noise. This noise then quanitifes the degree of uncertainty in our estimates, allowing the construction of test statistics and hypothesis tests about the true value of the population parameters. This sequence is illustrated in {numref}`modelling-fig`.\n",
    "\n",
    "```{figure} images/modelling.png\n",
    "---\n",
    "width: 800px\n",
    "name: modelling-fig\n",
    "---\n",
    "Illustration of how a statistical model splits a dataset into the effects of interest and error.\n",
    "```   \n",
    "\n",
    "As well as the quantification of *consistent* effects in the data, statistical modelling of fMRI data has another distinct advantage. To see this, consider an fMRI dataset with spatial dimensions $50 \\times 60 \\times 50$ and a temporal dimension of $300$ time points. Within this dataset there will $50 \\times 60 \\times 50 \\times 300 = 45,000,000$ data points. This is a huge amount of data. However, given that a statistical model will typically have many fewer parameters than data points, we can use modelling to *simplify* the data into only the effects of most interest. For instance, in a basic model we may be able to summarise the effects within each measured time series in terms of only *two* parameters. This reduces the amount of information from $45,000,000$ data points to just $300,000$. While still a large number, this is much more manageable. We can then use hypothesis testing to reduce this down even further into only those effects that are *significant*. As such, statistical modelling can be used to not only summarise the data within each time series, but can also be used to highlight where in the image those summaries are of most interest. This can be thought of as a processes of *summarising*, *refining* and *simplifying* the data.    \n",
    "\n",
    "```{admonition} Understanding the term *model*\n",
    ":class: tip\n",
    "Within the context of a statistical analysis, the term *model* can be thought of in a similar fashion to its everyday use. In much the same way that a *model* is a simplified replication of something, a statistical model is a simplification of reality. In this sense, a model is not designed to create an *exact* prediction of the phenomena under study. Instead, a good model captures the general patterns in the relationships between the variables of interest, producing a prediction that is *close enough* to have some utility. As such, probability distributions and their parameters are not expected to be *perfect* descriptions of every phenomena, but they are expected to capture a degree of the behaviour of real-world phenomena. As the statistician George Box stated: \"All models are wrong, but some are useful\". \n",
    "```\n",
    "\n",
    "## How Do We Model fMRI Data?\n",
    "In terms of *how* this process of simplification is achieved, recall that each voxel from one volume of an fMRI series represents a single point in a BOLD time series. This time series reflects how the measured signal changed during the course of the experiment. As illustrated in {numref}`timeseries-fig`, there is therefore a time series at every single spatial location with an image.\n",
    "\n",
    "```{figure} images/time-series-everywhere.png\n",
    "---\n",
    "width: 800px\n",
    "name: timeseries-fig\n",
    "---\n",
    "Illustration of how each voxel of an image is associated with a time series of BOLD signal change.\n",
    "```\n",
    "\n",
    "Using the Statistical Parametric Mapping (SPM) approach to modelling fMRI data, a statistical model is specified and estimated at *each time series separately*. This is known as the *mass-univariate* approach to modelling neuroimaging data. From the example above, this means specifiying and estimating $50 \\times 60 \\times 50 = 150,000$ models for every subject. The estimated parameters from those models then provide an indication about the *magnitude* and *direction* of the experimental effects at *every voxel*. Hypothesis tests can then be used to determine which voxels contain experimental effects that are *significant*, which can then be displayed as an overlay of activation clusters for localisation and interpretation. An example is shown in {numref}`threshold-fig`.\n",
    "\n",
    "```{figure} images/threshold.png\n",
    "---\n",
    "width: 800px\n",
    "name: threshold-fig\n",
    "---\n",
    "Illustration of how the results of hypothesis tests at each voxel can be thresholded based on statistical significance and then used to localise the experimental effects..\n",
    "```\n",
    "\n",
    "The mathematical framework used for statistical modelling, estimation and inference on fMRI data is known as the *General Linear Model* (GLM) and will be the main focus of this lesson. In the next section, we will see the general theory of the GLM before seeing it applied to a non-imaging dataset in the section after. Once this is understood, the application of the GLM to modelling an fMRI time series will be much easier to digest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b384cc6-4b55-421f-a641-8feb68181cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATLAB Kernel",
   "language": "matlab",
   "name": "jupyter_matlab_kernel"
  },
  "language_info": {
   "file_extension": ".m",
   "mimetype": "text/x-matlab",
   "name": "matlab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
