{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2c68e8-6e0f-4c4a-a960-21041c6a2ba3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The General Linear Model\n",
    "To begin with, we will take a high-level perspective on the GLM as a *framework* that can be applied to any dataset. This may appear somewhat abstract on a first read, however, we will see how this is applied to a real dataset in the next section. Remember that at this point we are not talking about fMRI data specifically. Given that this is a specialised application of the GLM, we first need to discuss the general theory before seeing how this traditional approach needs to be adjusted to suit modelling an fMRI time series.\n",
    "\n",
    "## The General Linear Model Framework\n",
    "To begin with, we will discuss the mathematical framework behind the GLM. In order to understand the GLM, we need to understand *multiple regression* which, in its most basic form, is know as *simple* regression.\n",
    "\n",
    "### Simple Regression\n",
    "In a simple regression model there is a single outcome variable $y$ that is associated with a single predictor variable $x$. The simple regression model takes the form\n",
    "\n",
    "$$\n",
    "y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}\n",
    "$$\n",
    "\n",
    "which defines a straight-line fit to the data, with $\\beta_{0}$ representing the *intercept* and $\\beta_{1}$ representing the *slope*. The term $\\epsilon$ quantifies the amount of *error* in the model, allowing for the fact that a perfect fit between the two variables is rarely possible. This model is illustrated in {numref}`simple-fig`. In this example, the manager of a production line wants a good estimate of the required number of worker hours given the number of units that must be produced (the *lot size*). \n",
    "\n",
    "```{figure} images/simple-reg.png\n",
    "---\n",
    "width: 700px\n",
    "name: simple-fig\n",
    "---\n",
    "Illustration of how a simple regression model amounts to constructing a straight-line to summarise the relationship between the predictor variable and the outcome variable.\n",
    "```\n",
    "\n",
    "As we can see, the simple regression model consists of a straight line through the scatterplot of measurements. For each value of `Lot Size`, the point on the regression line represents the predicted value of `Hours`. The magnitude of the estimated slope is therefore of interest, given that this quantifies the *strength* of the general relationship between the two variables. However, we also need to consider how close the raw data sits to the regression line as data that are tightly-packed around the regression line suggest a model that fits the data well. This concept of *model fit* is therefore quantified by the *errors*. The smaller that the $\\epsilon$ terms are, the shorter the vertical distances between the regression line and the raw data.  \n",
    "\n",
    "These concepts can be further understood by considering the probability model for a simple regression\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}(\\beta_{0} + \\beta_{1}x_{i}, \\sigma)\n",
    "$$\n",
    "\n",
    "Taking the example from {numref}`simple-fig`, each value of `Lot Size` is therefore associated with a normal distribution of values for `Hours` where the means sit along the population regression line. An illustration of this concept is given in {numref}`simple-prob-fig`.\n",
    "\n",
    "```{figure} images/simple-reg-prob.gif\n",
    "---\n",
    "width: 700px\n",
    "name: simple-prob-fig\n",
    "---\n",
    "Illustration of the simple regression normal probability model. The operator $E(Y)$ indicates the *expected* value of the outcome variable which, for a normal distribution, is equivalent to the mean.\n",
    "```\n",
    "\n",
    "The standard deviation of these normal distributions is given by $\\sigma$ and reflects how widely spread the data are around the regression line. There is therefore a direct connection between the width of the assumed probability distributions and the model errors. As such, for each value of `Lot Size`, we expect there to be a normal distribution of errors, spread equally above and below the line, with most errors close to the line and fewer further away. As such, the simple regression model can also be expressed as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{i} &= \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\\\\n",
    "\\epsilon_{i} &\\sim \\mathcal{N}(0,\\sigma)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Remembering that this probability model represents the *population*, the aim of a simple regression analysis is to use a *sample* to estimate both the *mean* and *variance* of the population distribution. As the mean depends upon the parameters $\\beta_{0}$ and $\\beta_{1}$, these must also be estimated from the sample. Finding the line that best fits the data will result in estimates for $\\beta_{0}$ and $\\beta_{1}$. The errors resulting from this fit can then be used to estimate $\\sigma$. \n",
    "\n",
    "Once the parameters are estimated, hypothesis testing can be used to determine whether the null hypotheses\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{H}_{0}:& \\beta_{0} = 0 \\\\\n",
    "\\mathcal{H}_{0}:& \\beta_{1} = 0\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "can be rejected. Usually, the test on $\\beta_{1}$ is of most interest because a slope of 0 is indicative of *no* relationship between the outcome and predictor variables. If significant, we would assume some non-zero relationship is present in the population and can interpret what the magnitude of $\\hat{\\beta}_{1}$ tells us about the relationship under study. This is often in the context of using the simple regression model to *predict* the outcome using the values of the predictor. For the example above, this would result in being able to predict `Hours` by only knowing the value of `Lot Size`, allowing the manager to determine if more or fewer workers need to be hired as production demands are scaled up or down.\n",
    "\n",
    "```{admonition} Randomness of the errors\n",
    ":class: tip\n",
    "One of the more confusing aspects of statistical models is that they can be written in mutliple equivalent ways. For instance, for simple regression, the probability model can be written as either\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}\\left(\\beta_{0} + \\beta_{1}x_{i},\\sigma\\right)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{i} &= \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\\\\n",
    "\\epsilon_{i} &\\sim \\mathcal{N}(0,\\sigma)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Although the first form may be more intuitive, the second form is insightful in terms of the principles of these models. Recall that we are assuming that there is some *constant* effect that runs through all our observations of a particular phenomena. Also recall that we assume that the reason we do not observe the same value every time is because we are measuring a *random process* that is subject to measurement error. As such, each measurement we take is the sum of our constant effect of interest and random perturbations. This structure is then reflected in the statistical model, as the mean of the assumed distribution is considered the *constant* effect and the errors are considered the *random* deflections. As such, the *errors* are the element that creates randomness and thus are the *random variable* component of our measurements. In statistical parlance, this is the distinction between *fixed-effects* and *random-effects*. Fixed-effects are associated with the mean of the assumed probability distribution, whereas random-effects are associated with random perturbations and thus the *variance* of the measurements.\n",
    "\n",
    "```\n",
    "\n",
    "### Multiple Regression\n",
    "Expanding the simple regression model to contain *multiple* predictor variables produces the multiple regression model. As such, the multiple regression model with $k$ predictor variables is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{i} &= \\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} + \\dots + \\beta_{k}x_{ik} + \\epsilon_{i} \\\\\n",
    "&= \\beta_{0} + \\sum_{j=1}^{k} \\beta_{j}x_{ij} + \\epsilon_{i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we usually assume a normal probability model of the form\n",
    "\n",
    "$$\n",
    "y_{i} \\sim \\mathcal{N}\\left(\\beta_{0} + \\sum_{j=1}^{k} \\beta_{j}x_{ij},\\sigma\\right)\n",
    "$$\n",
    "\n",
    "which can also be expressed as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{i} &= \\beta_{0} + \\sum_{j=1}^{k} \\beta_{j}x_{ij} + \\epsilon_{i} \\\\\n",
    "\\epsilon_{i} &\\sim \\mathcal{N}\\left(0,\\sigma\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Conceptually, much of multiple regression is the same as simple regression, except for a few details. Firstly, rather than a regression line this model estimates a regression *plane* in $k$-dimensional space. For instance, if $k=2$ then the model can be visualised as shown in {numref}`plane-fig`\n",
    "\n",
    "```{figure} images/reg-plane.png\n",
    "---\n",
    "width: 600px\n",
    "name: plane-fig\n",
    "---\n",
    "Illustration of how a multiple regression model with $k$ predictors forms a regression *plane* through $k$-dimensional space.\n",
    "```\n",
    "\n",
    "The means of the assumed normal distributions are no longer points along a regression line, rather they become points on a plane in $k$-dimensional space. The individual regression slopes for each predictor $\\left(\\beta_{j}\\right)$ are given by the *edges* of this plane (where $j = 1,\\dots,k$). Importantly, these slopes are not the same as fitting a simple regression model to each predictor separately. The slope coefficients represent the effect of each predictor after taking all other predictors into account. This means that adding or altering the predictors will change *all* the parameter estimates. This is an important point because many statistical modelling decisions are built upon this fact. For instance, the notion of *controlling* for the effect of a variable is based directly upon this behaviour.\n",
    "\n",
    "Despite these differences, the multiple regression model proceeds in much the same fashion as simple regression. The individual parameters of the mean function can be estimated to produce a single estimate for the intercept $\\left(\\beta_{0}\\right)$ and $k$ estimates for the slopes $\\left(\\beta_{1},\\dots,\\beta_{k}\\right)$. Estimation of the variance again proceeds using the errors, which now represent vertical distances from the regression *plane* to the raw data. Hypothesis tests can then be performed on each of the estimates to determine which of the $k$ variables appears to show a significant non-zero relationship with the outcome. This involves testing null hypotheses of the form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{H}_{0}:& \\beta_{0} = 0 \\\\\n",
    "\\mathcal{H}_{0}:& \\beta_{1} = 0 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{H}_{0}:& \\beta_{k} = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Much like simple regression, the general aim is the accurate *prediction* of the outcome, this time using value of *multiple* variables. For instance, the manager of the production line may enhance their model of `Hours` by also considering `Wage` and `Age` of the workers, alongside `Lot Size`. This may allow for a more accurate prediction of `Hours`, as well as allowing the manager to determine which factor is the most influential and where the focus should be when considering how to adapt to changes in production. \n",
    "\n",
    "### Multiple Regression in Matrix Form\n",
    "Now that we have discussed both *simple* and *multiple* regression, we can return to the topic of the GLM. The most important point to understand is that the GLM is simply *multiple regression in matrix form*. As such, if you understand multiple regression, then you already understand the GLM. The fact that there is a different term to refer to the matrix-variant of multiple regression is largely historical, as discussed in the box below. \n",
    "\n",
    "```{admonition} The history of the GLM\n",
    ":class: tip\n",
    "Historically, statistical analyses could be categorised as either *regression* methods or *analysis of variance* (ANOVA) methods. The distinction was that regression models sought to estimate the relationships between mutliple continuous measurements as a means of prediction, whereas ANOVA models sought to investigate group differences resulting from experimental manipulations. As such, regression was more related to observational studies, whereas ANOVA was associated with designed experiments. However, it was recognised in the 1950s that these two methods could be combined, meaning that an ANOVA model could be specified using mutliple regression. Since this point, the equivelance of these methods has been well-recognised, though regression and ANOVA are still often taught in isolation. The use of the term *General Linear Model* is usually reserved for the specific matrix-based framework using to specify both regression and ANOVA models, whereas the terms *regression* and *ANOVA* are typically used in reference to the more historical non-matrix forms of the same analyses.\n",
    "```\n",
    "\n",
    "In terms of writing multiple regression in matrix notation, we start with the observation that there are always $n$ regression equations, one for each data point\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{1} &= \\beta_{0} + \\beta_{1}x_{11} + \\beta_{2}x_{12} + \\dots + \\beta_{k}x_{1k} + \\epsilon_{1} \\\\\n",
    "y_{2} &= \\beta_{0} + \\beta_{1}x_{21} + \\beta_{2}x_{22} + \\dots + \\beta_{k}x_{2k} + \\epsilon_{2} \\\\\n",
    "y_{3} &= \\beta_{0} + \\beta_{1}x_{31} + \\beta_{2}x_{32} + \\dots + \\beta_{k}x_{3k} + \\epsilon_{3} \\\\\n",
    "\\vdots  \\\\\n",
    "y_{n} &= \\beta_{0} + \\beta_{1}x_{n1} + \\beta_{2}x_{n2} + \\dots + \\beta_{k}x_{nk} + \\epsilon_{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because these regression equations represent a system of linear equations, we know we can write them as vectors and matrices. Importantly, we can separate the predictor variables from the coefficients to give the following structure\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "y_{3} \\\\\n",
    "\\vdots \\\\\n",
    "y_{n}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1      & x_{11} & x_{12}  & \\dots  & x_{1k} \\\\\n",
    "1      & x_{21} & x_{22}  & \\dots  & x_{2k} \\\\\n",
    "1      & x_{31} & x_{32}  & \\dots  & x_{3k} \\\\\n",
    "\\vdots & \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
    "1      & x_{n1} & x_{n2}  & \\dots  & x_{nk} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_{0} \\\\\n",
    "\\beta_{1} \\\\\n",
    "\\beta_{2} \\\\\n",
    "\\vdots    \\\\\n",
    "\\beta_{k}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\epsilon_{1} \\\\\n",
    "\\epsilon_{2} \\\\\n",
    "\\epsilon_{3} \\\\\n",
    "\\vdots       \\\\\n",
    "\\epsilon_{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This can be written in shorthand as\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "This is probably the most important equation you will see on this course. It encapsulates the entire structure of our data analysis and is something you will become intimately familiar with during this module. To make this structure clear \n",
    "- $\\mathbf{Y}$ represents the *data vector* containing the values of the outcome variable\n",
    "- $\\mathbf{X}$ represents the *design matrix* containing each of the predictor variables as columns \n",
    "- $\\boldsymbol{\\beta}$ represents the vector of *model parameters* \n",
    "- $\\boldsymbol{\\epsilon}$ represents the vector of *errors* \n",
    "\n",
    "Importantly, we need to recognise how multiplying $\\mathbf{X}$ and $\\boldsymbol{\\beta}$ recreates the $n$ regression equations.\n",
    "\n",
    "$$\n",
    "\\mathbf{X}\\boldsymbol{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "1      & x_{11} & x_{12}  & \\dots  & x_{1k} \\\\\n",
    "1      & x_{21} & x_{22}  & \\dots  & x_{2k} \\\\\n",
    "1      & x_{31} & x_{32}  & \\dots  & x_{3k} \\\\\n",
    "\\vdots & \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
    "1      & x_{n1} & x_{n2}  & \\dots  & x_{nk} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_{0} \\\\\n",
    "\\beta_{1} \\\\\n",
    "\\beta_{2} \\\\\n",
    "\\vdots    \\\\\n",
    "\\beta_{k}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\beta_{0} + \\beta_{1}x_{11} + \\beta_{2}x_{12} + \\dots  + \\beta_{k}x_{1k} \\\\\n",
    "\\beta_{0} + \\beta_{1}x_{21} + \\beta_{2}x_{22} + \\dots  + \\beta_{k}x_{2k} \\\\\n",
    "\\beta_{0} + \\beta_{1}x_{31} + \\beta_{2}x_{32} + \\dots  + \\beta_{k}x_{3k} \\\\\n",
    "\\vdots  \\\\\n",
    "\\beta_{0} + \\beta_{1}x_{n1} + \\beta_{2}x_{n2} + \\dots  + \\beta_{k}x_{nk}\n",
    "\\end{bmatrix}\n",
    "= \\hat{\\mathbf{Y}\n",
    "$$\n",
    "\n",
    "These equations produce the *predicted* values from the model $\\left(\\hat{\\mathbf{Y}}\\right)$. As such, another way to look at the GLM is as a *prediction* plus *error*\n",
    "\n",
    "$$\n",
    "\\underset{\\text{Data}}{\\mathbf{Y}} = \\underset{\\text{Prediction}}{\\hat{\\mathbf{Y}}} + \\underset{\\text{Error}}{\\boldsymbol{\\epsilon}}\n",
    "$$\n",
    "\n",
    "where the nature of the prediction depends upon the form of the design matrix.\n",
    "\n",
    "## Building the Design Matrix\n",
    "\n",
    "At the core of the GLM is the structure of the design matrix. This is the part of the GLM that changes from analysis-to-analysis and is the element that defines different models of our data. The *general* bit of the GLM is a reference to the fact that including continuous predictors in the design matrix will produce a regression model, whereas including categorical predictors will produce an ANOVA model. As we will come to see, the design matrix is such a key part of the GLM that SPM chooses to visualise this element as a means of communicating the form of GLM being used to model our fMRI data. As such, understanding how the design matrix can be structured is important. Given that the design matrix can contain a mixture of *continuous* and *categorical* variables, we must therefore understand how these different types of predictors can be accommodated.\n",
    "\n",
    "### Continuous Predictor Variables\n",
    "\n",
    "Any variable that is numeric and represents some sort of measurement is classified as a continuous predictor variable. This includes any quantifiable phenomena such as IQ, reaction time, score on a test, age, weight etc. These types of predictor variables are straightforward to use within the GLM because we simply add them verbatim to the design matrix as columns. As such, they do not require any form of special treatment to be used. We will see examples of these types of variables in the next section, when we look at an example GLM using some real-world data.\n",
    "\n",
    "### Categorical Predictor Variables\n",
    "\n",
    "Any variable that represents a form of grouping or category is classified as a categorical variable. This could include variables such as sex, patient group, blood type, ethnicity etc. Compared with continuous variables, categorical predictor variables are more complex to use with the GLM. Fundamentally, what we have to do is to turn these categories into numbers in order to put them in the design matrix. The way we do this is to form dummy variables. These are variables that have a value of 1 or 0 depending on the category. For instance, if the categories were male or female, we could assign values like so\n",
    "\n",
    "| Category  | Dummy Variable Value |\n",
    "| --------- | -------------------- |\n",
    "| Control   | 0                    |\n",
    "| Patient   | 1                    |\n",
    "\n",
    "If we have a variable with more than two categories, we can include more dummy variables like so\n",
    "\n",
    "|  Blood Type | Dummy 1 Value | Dummy 2 Value | Dummy 3 Value  |\n",
    "| :---------- | ------------- | ------------- | -------------: |\n",
    "|  A          | 1             | 0             | 0              |\n",
    "|  B          | 0             | 1             | 0              |\n",
    "|  AB         | 0             | 0             | 1              | \n",
    "|  O          | 0             | 0             | 0              |\n",
    "\n",
    "Notice how we always have one fewer dummy variable than the number of categories. So we only need 1 dummy variable to represent the 2 categories of male or female, and we only need 3 dummy variables to represent the 4 categories of blood type. As an example, we can see a design matrix below that contains a dummy variable representing diagnosis, where the first 3 subjects are patients and the last 3 are controls.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Any row that contains a 1 in the second column indicates that subject is female whereas any row that contains a 0 in the second column indicates that subject is male. We are going to see dummy variables used a lot and so do not worry if this concept is not clear yet. We will see plenty of examples as we press forward.\n",
    " \n",
    "## Estimating the Parameters\n",
    "\n",
    "Once we have constructed our design matrix, we have a fully-formed probability model of the form\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} \\sim \\mathcal{N}\\left(\\mathbf{X}\\boldsymbol{\\beta},\\sigma\\mathbf{I}\\right)\n",
    "$$\n",
    "\n",
    "The next step is therefore to use maximum likelihood methods to estimate the model parameters. The equations that are derived from the method of maximum likelihood can be written in matrix terms to give a single equation that will simultaneously estimate values for every parameter. This equation is given by\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{\\prime}\\mathbf{Y}\n",
    "$$\n",
    "\n",
    "and is notable for its use of a matrix inverse. As you may remember from the Computational Tools lesson in Functional Neuroanatomy, inverting a matrix is a tricky business because an inverse may not always exist. This tells us something about the limitations of the GLM, namely that not every model we may want to use will be estimable. This is a thorny issue, but one which is thankfully rarely a concern when using SPM.\n",
    "\n",
    "## Interpreting the Parameters\n",
    "\n",
    "Once we have the parameter estimates, our aim is to try and interpret what they mean, given that their values represents important summaries of the effects in our dataset. If the variable associated with a parameter estimate is a continuous variable, we would interpret the value as a regression slope, telling us how much our outcome variable is predicted to change for a unit increase in the predictor variable. For example, a regression slope with a value of -5.344 is visualised in {numref}`continuous-fig`\n",
    "\n",
    "```{figure} images/reg-continuous.png\n",
    "---\n",
    "width: 500px\n",
    "name: continuous-fig\n",
    "---\n",
    "Example of the regression slope associated with a continuous predictor variable and a parameter estimate of $\\beta_{1}=-5.344$.\n",
    "```\n",
    "\n",
    "This can be interpreted as a unit increase in the value of the predictor variable being associated with a decrease in the value of the outcome variable of 5.344. By comparison, if the variable associated with a parameter estimate is a categorical variable, we would interpret the value as a mean difference. To see why, consider what happens when we fit a regression slope to a dummy variable with a value of 0 or 1\n",
    "\n",
    "```{figure} images/reg-dummy.png\n",
    "---\n",
    "width: 500px\n",
    "name: dummy-fig\n",
    "---\n",
    "Example of the regression slope associated with a categorical predictor variable and a parameter estimate of $\\beta_{1}=7.940$.\n",
    "```\n",
    "\n",
    "So we can see that the model is still fitting a regression slope, but one that goes from the mean of one category to the mean of the other. In this example, the slope has a value of 7.94 which tells us the mean difference between the categories. The intercept of the model is the mean of the group coded as 0 and a unit change simply refers to the change from one group to the other. So although this is still the same interpretation as any other regression slope, it is useful to think of the slopes from dummy variables representing mean differences.\n",
    "\n",
    "Although we can interpret the values of our parameter estimates, on their own this not enough because we also need to know how much we can trust these estimates. This is done by calculating the standard deviation of the sampling distribution of the estimates, known as the standard error. In the GLM, the standard errors of the parameter estimates can be calculated using the estimate of the model variance\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^{2} = \\frac{\\boldsymbol{\\epsilon}^{\\prime}\\boldsymbol{\\epsilon}}{n-p}\n",
    "$$\n",
    "\n",
    "which is the squared estimate of the standard deviation of the normal distribution we are using for our model. Here, n is the number of rows of X and p is the number of model parameters (the number of columns of X). This is then combined with the design matrix to produce a variance-covariance matrix\n",
    "\n",
    "$$\n",
    "\\text{Cov}\\left(\\hat{\\boldsymbol{\\beta}}\\right) = \\hat{\\sigma}^{2}\\left(\\mathbf{X}^{\\prime}\\mathbf{X}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "You do not need to worry too much about what this represents. The main point is just that the standard errors are taken as the square-root of the diagonal elements of this matrix. \n",
    "\n",
    "## Inference\n",
    "\n",
    "... Although this seems fairly straightforward, inference using neuroimaging data has some very specific challenges that we will pick up on next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe3fd4-92e5-4758-9150-a387c1e43560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATLAB Kernel",
   "language": "matlab",
   "name": "jupyter_matlab_kernel"
  },
  "language_info": {
   "file_extension": ".m",
   "mimetype": "text/x-matlab",
   "name": "matlab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}